#!/bin/bash
# MySQL CDCè¿æ¥å™¨å®‰å…¨å®‰è£…è„šæœ¬
# ================================
# 
# ç”¨é€”: åœ¨ç”Ÿäº§ç¯å¢ƒå®‰å…¨å®‰è£…MySQL CDCè¿æ¥å™¨
# ç‰¹æ€§: æ£€æŸ¥ç‚¹ä¿å­˜ã€ä½œä¸šæ¢å¤ã€å›æ»šä¿æŠ¤
# ä½œè€…: Flinkè¿ç»´å›¢é˜Ÿ
# æ›´æ–°: 2025-05-29

set -e  # é‡é”™å³åœ

# é…ç½®å˜é‡
FLINK_HOME="/home/ubuntu/flink"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
BACKUP_DIR="/tmp/flink_backup_$(date +%Y%m%d_%H%M%S)"
CDC_JAR="/tmp/flink-sql-connector-mysql-cdc-3.2.1.jar"
FLINK_WEB="http://localhost:8081"

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $1"
}

# æ£€æŸ¥å‰ç½®æ¡ä»¶
check_prerequisites() {
    log_info "å¼€å§‹æ£€æŸ¥å‰ç½®æ¡ä»¶..."
    
    # æ£€æŸ¥CDC jaræ–‡ä»¶
    if [ ! -f "$CDC_JAR" ]; then
        log_error "MySQL CDCè¿æ¥å™¨æ–‡ä»¶ä¸å­˜åœ¨: $CDC_JAR"
        log_info "è¯·å…ˆè¿è¡Œ: wget https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-mysql-cdc/3.2.1/flink-sql-connector-mysql-cdc-3.2.1.jar -O $CDC_JAR"
        exit 1
    fi
    
    # æ£€æŸ¥Flinkç›®å½•
    if [ ! -d "$FLINK_HOME" ]; then
        log_error "Flinkç›®å½•ä¸å­˜åœ¨: $FLINK_HOME"
        exit 1
    fi
    
    # æ£€æŸ¥Flinké›†ç¾¤çŠ¶æ€
    if ! curl -s "$FLINK_WEB/jobs" >/dev/null; then
        log_error "æ— æ³•è¿æ¥Flinké›†ç¾¤: $FLINK_WEB"
        exit 1
    fi
    
    log_success "å‰ç½®æ¡ä»¶æ£€æŸ¥é€šè¿‡"
}

# è·å–å½“å‰è¿è¡Œçš„ä½œä¸š
get_running_jobs() {
    log_info "è·å–å½“å‰è¿è¡Œçš„ä½œä¸šåˆ—è¡¨..."
    
    JOBS=$(curl -s "$FLINK_WEB/jobs" | python3 -c "
import sys, json
data = json.load(sys.stdin)
for job in data.get('jobs', []):
    if job['status'] == 'RUNNING':
        print(f'{job[\"id\"]}:{job[\"status\"]}')
")
    
    if [ -z "$JOBS" ]; then
        log_info "å½“å‰æ²¡æœ‰è¿è¡Œçš„ä½œä¸š"
    else
        log_info "å½“å‰è¿è¡Œçš„ä½œä¸š:"
        echo "$JOBS" | while read job; do
            echo "  - $job"
        done
    fi
    
    echo "$JOBS"
}

# åˆ†æå½“å‰è¿è¡Œçš„ä½œä¸šè¯¦æƒ…
analyze_running_jobs() {
    local jobs="$1"
    log_info "åˆ†æå½“å‰è¿è¡Œä½œä¸šçš„è¯¦ç»†ä¿¡æ¯..."
    
    mkdir -p "$BACKUP_DIR/job_analysis"
    
    echo "# å½“å‰è¿è¡Œä½œä¸šåˆ†ææŠ¥å‘Š - $(date)" > "$BACKUP_DIR/job_analysis/job_details.txt"
    echo "# ==========================================" >> "$BACKUP_DIR/job_analysis/job_details.txt"
    echo "" >> "$BACKUP_DIR/job_analysis/job_details.txt"
    
    echo "$jobs" | while read job_line; do
        if [ -n "$job_line" ]; then
            job_id=$(echo "$job_line" | cut -d':' -f1)
            log_info "åˆ†æä½œä¸š: $job_id"
            
            # è·å–ä½œä¸šè¯¦ç»†ä¿¡æ¯
            job_detail=$(curl -s "$FLINK_WEB/jobs/$job_id")
            echo "$job_detail" > "$BACKUP_DIR/job_analysis/${job_id}_detail.json"
            
            # è§£æä½œä¸šåç§°å’Œç±»å‹
            job_info=$(echo "$job_detail" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    job_name = data.get('name', 'Unknown')
    vertices = data.get('vertices', [])
    
    print(f'ä½œä¸šID: ${job_id}')
    print(f'ä½œä¸šåç§°: {job_name}')
    print(f'é¡¶ç‚¹æ•°é‡: {len(vertices)}')
    
    # åˆ†æä½œä¸šç±»å‹
    job_type = 'Unknown'
    if 'kafka' in job_name.lower():
        job_type = 'Kafkaä½œä¸š'
    elif 'mysql' in job_name.lower() or 'cdc' in job_name.lower():
        job_type = 'MySQL CDCä½œä¸š'
    elif 'doris' in job_name.lower():
        job_type = 'Dorisç›¸å…³ä½œä¸š'
    
    print(f'æ¨æµ‹ç±»å‹: {job_type}')
    
    # æŸ¥æ‰¾sourceå’Œsinkä¿¡æ¯
    for vertex in vertices:
        vertex_name = vertex.get('name', '')
        if 'source' in vertex_name.lower():
            print(f'Source: {vertex_name}')
        elif 'sink' in vertex_name.lower():
            print(f'Sink: {vertex_name}')
except Exception as e:
    print(f'è§£æå¤±è´¥: {e}')
")
            
            echo "ä½œä¸š $job_id:" >> "$BACKUP_DIR/job_analysis/job_details.txt"
            echo "$job_info" | sed 's/^/  /' >> "$BACKUP_DIR/job_analysis/job_details.txt"
            echo "" >> "$BACKUP_DIR/job_analysis/job_details.txt"
            
            # å°è¯•åŒ¹é…å¯èƒ½çš„SQLæ–‡ä»¶
            possible_sql=$(echo "$job_detail" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    job_name = data.get('name', '').lower()
    vertices = data.get('vertices', [])
    
    # æ ¹æ®ä½œä¸šç‰¹å¾æ¨æ–­å¯èƒ½çš„SQLæ–‡ä»¶
    suggestions = []
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯Kafkaä½œä¸š
    has_kafka_source = any('kafka' in v.get('name', '').lower() for v in vertices)
    has_doris_sink = any('doris' in v.get('name', '').lower() for v in vertices)
    
    if has_kafka_source and has_doris_sink:
        suggestions.append('flink_app/kafka2doris/scripts/kafka_to_doris_production.sql')
        suggestions.append('flink_app/kafka2doris/scripts/kafka_to_doris_solution_sample.sql')
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯MySQL CDCä½œä¸š
    has_mysql_source = any('mysql' in v.get('name', '').lower() for v in vertices)
    if has_mysql_source:
        suggestions.append('flink_app/mysql2doris/scripts/mysql_sync.sql')
        suggestions.append('flink_app/mysql2doris/scripts/mysql_content_audit_to_doris.sql')
    
    for suggestion in suggestions:
        print(suggestion)
        
except:
    pass
")
            
            if [ -n "$possible_sql" ]; then
                echo "  å¯èƒ½çš„SQLæ–‡ä»¶:" >> "$BACKUP_DIR/job_analysis/job_details.txt"
                echo "$possible_sql" | sed 's/^/    - /' >> "$BACKUP_DIR/job_analysis/job_details.txt"
            fi
            echo "" >> "$BACKUP_DIR/job_analysis/job_details.txt"
        fi
    done
    
    log_success "ä½œä¸šåˆ†æå®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜åˆ°: $BACKUP_DIR/job_analysis/"
}

# ç®€åŒ–çš„SQLå†…å®¹åŒ¹é…åŠŸèƒ½
extract_and_match_sql() {
    local jobs="$1"
    log_info "æå–å½“å‰ä½œä¸šä¿¡æ¯å¹¶è¿›è¡ŒSQLåŒ¹é…..."
    
    mkdir -p "$BACKUP_DIR/sql_analysis"
    
    echo "# SQLåŒ¹é…åˆ†ææŠ¥å‘Š - $(date)" > "$BACKUP_DIR/sql_analysis/sql_match_report.txt"
    echo "# =================================" >> "$BACKUP_DIR/sql_analysis/sql_match_report.txt"
    echo "" >> "$BACKUP_DIR/sql_analysis/sql_match_report.txt"
    
    echo "$jobs" | while read job_line; do
        if [ -n "$job_line" ]; then
            job_id=$(echo "$job_line" | cut -d':' -f1)
            log_info "åˆ†æä½œä¸š $job_id..."
            
            # è·å–ä½œä¸šè¯¦ç»†ä¿¡æ¯
            job_detail=$(curl -s "$FLINK_WEB/jobs/$job_id")
            echo "$job_detail" > "$BACKUP_DIR/sql_analysis/${job_id}_detail.json"
            
            # è·å–ä½œä¸šæ‰§è¡Œè®¡åˆ’
            job_plan=$(curl -s "$FLINK_WEB/jobs/$job_id/plan")
            echo "$job_plan" > "$BACKUP_DIR/sql_analysis/${job_id}_plan.json"
            
            # ä»æ‰§è¡Œè®¡åˆ’å’Œä½œä¸šåç§°ä¸­æå–å…³é”®ç‰¹å¾
            job_features=$(echo "$job_plan" | python3 -c "
import sys, json, re

try:
    data = json.load(sys.stdin)
    plan_nodes = data.get('plan', {}).get('nodes', [])
    
    features = set()
    
    for node in plan_nodes:
        description = node.get('description', '')
        
        # æå–æºè¡¨ä¿¡æ¯
        if 'TableSourceScan' in description:
            table_match = re.search(r'table=\[\[default_catalog, default_database, ([^,\]]+)', description)
            if table_match:
                table_name = table_match.group(1)
                features.add(f'source_table:{table_name}')
        
        # æŸ¥æ‰¾è¿æ¥å™¨ç›¸å…³çš„å…³é”®è¯
        if 'kafka' in description.lower():
            features.add('connector:kafka')
        if 'mysql' in description.lower() or 'cdc' in description.lower():
            features.add('connector:mysql-cdc')
        if 'doris' in description.lower():
            features.add('connector:doris')
        if 'filesystem' in description.lower() or 'Writer' in description:
            features.add('connector:filesystem')
    
    print('|'.join(sorted(features)))
    
except Exception as e:
    print(f'è§£æé”™è¯¯: {e}')
")
            
            # ä»ä½œä¸šåç§°ä¸­æå–ç›®æ ‡è¡¨ä¿¡æ¯
            job_name=$(echo "$job_detail" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    job_name = data.get('name', '')
    if 'insert-into_default_catalog.default_database.' in job_name:
        sink_table = job_name.split('.')[-1]
        print(f'sink_table:{sink_table}')
except:
    print('')
")

            # åˆå¹¶ç‰¹å¾
            if [ -n "$job_name" ]; then
                job_features="$job_features|$job_name"
            fi
            
            # ä¿å­˜ä½œä¸šç‰¹å¾
            echo "ä½œä¸š $job_id ç‰¹å¾: $job_features" > "$BACKUP_DIR/sql_analysis/${job_id}_features.txt"
            
            # åŒ¹é…ç°æœ‰SQLæ–‡ä»¶
            log_info "åŒ¹é…ä½œä¸š $job_id ä¸ç°æœ‰SQLæ–‡ä»¶..."
            
            match_result=$(python3 -c "
import os, re

# ä½œä¸šç‰¹å¾
job_features = set('$job_features'.split('|')) if '$job_features' else set()

# å¢å¼ºçš„SQLæ–‡ä»¶ç‰¹å¾æå–
def extract_sql_features(sql_file):
    try:
        with open(sql_file, 'r', encoding='utf-8') as f:
            content = f.read()
    except:
        return set()
    
    features = set()
    
    # æ£€æŸ¥è¿æ¥å™¨ç±»å‹
    if \"'connector' = 'kafka'\" in content:
        features.add('connector:kafka')
    if \"'connector' = 'mysql-cdc'\" in content:
        features.add('connector:mysql-cdc')
    if \"'connector' = 'doris'\" in content:
        features.add('connector:doris')
    if \"'connector' = 'filesystem'\" in content:
        features.add('connector:filesystem')
    
    # æ£€æŸ¥æºè¡¨åï¼ˆCREATE TABLEï¼‰
    source_table_matches = re.findall(r'CREATE TABLE (\w+)', content, re.IGNORECASE)
    for table in source_table_matches:
        features.add(f'source_table:{table}')
    
    # æ£€æŸ¥ç›®æ ‡è¡¨åï¼ˆINSERT INTOï¼‰
    sink_table_matches = re.findall(r'INSERT INTO (\w+)', content, re.IGNORECASE)
    for table in sink_table_matches:
        features.add(f'sink_table:{table}')
    
    return features

# å€™é€‰SQLæ–‡ä»¶
sql_files = [
    '/home/ubuntu/work/script/flink_app/kafka2doris/scripts/kafka_to_doris_production.sql',
    '/home/ubuntu/work/script/flink_app/kafka2doris/scripts/kafka_to_doris_solution_sample.sql',
    '/home/ubuntu/work/script/flink_app/mysql2doris/scripts/mysql_sync.sql',
    '/home/ubuntu/work/script/flink_app/mysql2doris/scripts/mysql_content_audit_to_doris.sql'
]

# è®¡ç®—åŒ¹é…åº¦
matches = []
for sql_file in sql_files:
    if os.path.exists(sql_file):
        sql_features = extract_sql_features(sql_file)
        
        if job_features and sql_features:
            intersection = job_features.intersection(sql_features)
            union = job_features.union(sql_features)
            
            # å¯¹sink_tableåŒ¹é…ç»™äºˆæ›´é«˜æƒé‡
            sink_table_matches = len([f for f in intersection if f.startswith('sink_table:')])
            if sink_table_matches > 0:
                # å¦‚æœsinkè¡¨åŒ¹é…ï¼Œç»™é¢å¤–åŠ åˆ†
                score = (len(intersection) + sink_table_matches * 0.5) / len(union) if union else 0
            else:
                score = len(intersection) / len(union) if union else 0
        else:
            score = 0
        
        if score > 0:
            matches.append((sql_file, score, sorted(sql_features)))

# æ’åºå¹¶è¾“å‡º
matches.sort(key=lambda x: x[1], reverse=True)

print(f'ä½œä¸šç‰¹å¾: {sorted(job_features)}')
print('---')
for sql_file, score, sql_features in matches:
    print(f'{sql_file}:{score:.2f}:{sql_features}')
")
            
            echo "ä½œä¸š $job_id åŒ¹é…åˆ†æ:" >> "$BACKUP_DIR/sql_analysis/sql_match_report.txt"
            echo "$match_result" >> "$BACKUP_DIR/sql_analysis/sql_match_report.txt"
            echo "" >> "$BACKUP_DIR/sql_analysis/sql_match_report.txt"
            
            # ä¿å­˜åŒ¹é…ç»“æœ
            echo "$match_result" > "$BACKUP_DIR/sql_analysis/${job_id}_match.txt"
            
            # ä¿å­˜ä½œä¸šè¯¦ç»†æè¿°ç”¨äºæ‰‹åŠ¨é‡å»º
            job_description=$(echo "$job_detail" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    vertices = data.get('vertices', [])
    print('ä½œä¸šæ„æˆ:')
    for vertex in vertices:
        print(f'  - {vertex.get(\"name\", \"Unknown\")}')
except:
    print('æ— æ³•è§£æä½œä¸šæè¿°')
")
            echo "$job_description" > "$BACKUP_DIR/sql_analysis/${job_id}_description.txt"
        fi
    done
    
    log_success "SQLåŒ¹é…åˆ†æå®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜åˆ°: $BACKUP_DIR/sql_analysis/"
}

# åˆ›å»ºsavepoint
create_savepoints() {
    local jobs="$1"
    log_info "ä¸ºè¿è¡Œä¸­çš„ä½œä¸šåˆ›å»ºsavepoint..."
    
    mkdir -p "$BACKUP_DIR/savepoints"
    
    echo "$jobs" | while read job_line; do
        if [ -n "$job_line" ]; then
            job_id=$(echo "$job_line" | cut -d':' -f1)
            log_info "ä¸ºä½œä¸š $job_id åˆ›å»ºsavepoint..."
            
            # è§¦å‘savepointåˆ›å»º
            savepoint_result=$(curl -s -X POST "$FLINK_WEB/jobs/$job_id/savepoints" \
                -H "Content-Type: application/json" \
                -d '{"target-directory": "'$BACKUP_DIR'/savepoints", "cancel-job": false}')
            
            echo "$savepoint_result" > "$BACKUP_DIR/savepoints/${job_id}_trigger.json"
            log_info "Savepointåˆ›å»ºè¯·æ±‚å·²å‘é€: $job_id"
            
            # ç­‰å¾…savepointå®Œæˆå¹¶è®°å½•è·¯å¾„
            sleep 5
            savepoint_status=$(curl -s "$FLINK_WEB/jobs/$job_id/savepoints")
            echo "$savepoint_status" > "$BACKUP_DIR/savepoints/${job_id}_status.json"
        fi
    done
    
    log_info "ç­‰å¾…savepointåˆ›å»ºå®Œæˆ..."
    sleep 10
    
    # è®°å½•æ¢å¤æŒ‡ä»¤
    echo "# Flinkä½œä¸šæ¢å¤å‘½ä»¤ - $(date)" > "$BACKUP_DIR/recovery_commands.sh"
    echo "# ================================" >> "$BACKUP_DIR/recovery_commands.sh"
    echo "" >> "$BACKUP_DIR/recovery_commands.sh"
    
    echo "$jobs" | while read job_line; do
        if [ -n "$job_line" ]; then
            job_id=$(echo "$job_line" | cut -d':' -f1)
            
            # å°è¯•ä»çŠ¶æ€æ–‡ä»¶ä¸­æå–savepointè·¯å¾„
            if [ -f "$BACKUP_DIR/savepoints/${job_id}_status.json" ]; then
                savepoint_path=$(cat "$BACKUP_DIR/savepoints/${job_id}_status.json" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    if 'savepoints' in data and len(data['savepoints']) > 0:
        latest = data['savepoints'][-1]
        if 'location' in latest:
            print(latest['location'])
except:
    pass
" 2>/dev/null || echo "")
            
                if [ -n "$savepoint_path" ]; then
                    echo "# æ¢å¤ä½œä¸š $job_id" >> "$BACKUP_DIR/recovery_commands.sh"
                    echo "flink run -s $savepoint_path <åŸä½œä¸šjarè·¯å¾„>" >> "$BACKUP_DIR/recovery_commands.sh"
                    echo "" >> "$BACKUP_DIR/recovery_commands.sh"
                else
                    echo "# æ¢å¤ä½œä¸š $job_id (éœ€è¦æ‰‹åŠ¨æŸ¥æ‰¾savepointè·¯å¾„)" >> "$BACKUP_DIR/recovery_commands.sh"
                    echo "# åœ¨ $BACKUP_DIR/savepoints/ ç›®å½•æŸ¥æ‰¾savepoint" >> "$BACKUP_DIR/recovery_commands.sh"
                    echo "flink run -s <savepoint-path> <åŸä½œä¸šjarè·¯å¾„>" >> "$BACKUP_DIR/recovery_commands.sh"
                    echo "" >> "$BACKUP_DIR/recovery_commands.sh"
                fi
            fi
        fi
    done
}

# åœæ­¢ä½œä¸š
stop_jobs() {
    local jobs="$1"
    log_info "åœæ­¢è¿è¡Œä¸­çš„ä½œä¸š..."
    
    echo "$jobs" | while read job_line; do
        if [ -n "$job_line" ]; then
            job_id=$(echo "$job_line" | cut -d':' -f1)
            log_info "åœæ­¢ä½œä¸š: $job_id"
            
            # ä½¿ç”¨cancelæ¥åœæ­¢ä½œä¸šï¼ˆä¿æŒsavepointï¼‰
            curl -s -X PATCH "$FLINK_WEB/jobs/$job_id" >/dev/null
        fi
    done
    
    # ç­‰å¾…ä½œä¸šå®Œå…¨åœæ­¢
    log_info "ç­‰å¾…ä½œä¸šåœæ­¢..."
    sleep 5
}

# å¤‡ä»½å½“å‰é…ç½®
backup_config() {
    log_info "å¤‡ä»½å½“å‰Flinké…ç½®..."
    
    mkdir -p "$BACKUP_DIR/config"
    cp -r "$FLINK_HOME/conf"/* "$BACKUP_DIR/config/"
    
    if [ -d "$FLINK_HOME/lib" ]; then
        mkdir -p "$BACKUP_DIR/lib"
        cp "$FLINK_HOME/lib"/*.jar "$BACKUP_DIR/lib/" 2>/dev/null || true
    fi
    
    log_success "é…ç½®å¤‡ä»½å®Œæˆ: $BACKUP_DIR"
}

# å®‰è£…MySQL CDCè¿æ¥å™¨
install_connector() {
    log_info "å®‰è£…MySQL CDCè¿æ¥å™¨..."
    
    # å¤åˆ¶è¿æ¥å™¨åˆ°libç›®å½•
    cp "$CDC_JAR" "$FLINK_HOME/lib/"
    
    # éªŒè¯æ–‡ä»¶
    if [ -f "$FLINK_HOME/lib/flink-sql-connector-mysql-cdc-3.2.1.jar" ]; then
        log_success "è¿æ¥å™¨å®‰è£…æˆåŠŸ"
    else
        log_error "è¿æ¥å™¨å®‰è£…å¤±è´¥"
        exit 1
    fi
}

# é‡å¯Flinké›†ç¾¤
restart_cluster() {
    log_info "é‡å¯Flinké›†ç¾¤..."
    
    # åœæ­¢é›†ç¾¤
    log_info "åœæ­¢Flinké›†ç¾¤..."
    "$FLINK_HOME/bin/stop-cluster.sh"
    
    # ç­‰å¾…å®Œå…¨åœæ­¢
    sleep 5
    
    # å¯åŠ¨é›†ç¾¤
    log_info "å¯åŠ¨Flinké›†ç¾¤..."
    "$FLINK_HOME/bin/start-cluster.sh"
    
    # ç­‰å¾…é›†ç¾¤å°±ç»ª
    log_info "ç­‰å¾…é›†ç¾¤å¯åŠ¨..."
    sleep 10
    
    # éªŒè¯é›†ç¾¤çŠ¶æ€
    if curl -s "$FLINK_WEB/jobs" >/dev/null; then
        log_success "Flinké›†ç¾¤é‡å¯æˆåŠŸ"
    else
        log_error "Flinké›†ç¾¤å¯åŠ¨å¤±è´¥"
        exit 1
    fi
}

# éªŒè¯è¿æ¥å™¨å®‰è£…
verify_installation() {
    log_info "éªŒè¯MySQL CDCè¿æ¥å™¨å®‰è£…..."
    
    cd "$SCRIPT_DIR"
    if python3 config_validator.py --job mysql2doris --table user_interests; then
        log_success "MySQL CDCè¿æ¥å™¨éªŒè¯æˆåŠŸ"
    else
        log_error "MySQL CDCè¿æ¥å™¨éªŒè¯å¤±è´¥"
        return 1
    fi
}

# ä¸»å®‰è£…æµç¨‹
main_install() {
    log_info "å¼€å§‹MySQL CDCè¿æ¥å™¨å®‰è£…æµç¨‹..."
    echo
    
    # 1. æ£€æŸ¥å‰ç½®æ¡ä»¶
    check_prerequisites
    echo
    
    # 2. è·å–è¿è¡Œä¸­çš„ä½œä¸š
    RUNNING_JOBS=$(get_running_jobs)
    echo
    
    # 3. åˆ†æä½œä¸šè¯¦æƒ…
    analyze_running_jobs "$RUNNING_JOBS"
    echo
    
    # 4. æå–SQLå†…å®¹å¹¶è¿›è¡ŒåŒ¹é…
    extract_and_match_sql "$RUNNING_JOBS"
    echo
    
    # 5. ç”¨æˆ·ç¡®è®¤
    if [ -n "$RUNNING_JOBS" ]; then
        log_warn "æ£€æµ‹åˆ°è¿è¡Œä¸­çš„ä½œä¸šï¼Œå®‰è£…è¿‡ç¨‹å°†ä¼šåœæ­¢è¿™äº›ä½œä¸š"
        echo "ç»§ç»­å®‰è£…å°†ä¼š:"
        echo "  1. ä¸ºæ¯ä¸ªä½œä¸šåˆ›å»ºsavepoint"
        echo "  2. åœæ­¢æ‰€æœ‰è¿è¡Œä¸­çš„ä½œä¸š"
        echo "  3. é‡å¯Flinké›†ç¾¤"
        echo "  4. éœ€è¦æ‰‹åŠ¨æ¢å¤ä½œä¸š"
        echo
        read -p "æ˜¯å¦ç»§ç»­å®‰è£…? (y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            log_info "å®‰è£…å·²å–æ¶ˆ"
            exit 0
        fi
    fi
    
    # 6. å¤‡ä»½é…ç½®
    backup_config
    echo
    
    # 7. åˆ›å»ºsavepoint
    if [ -n "$RUNNING_JOBS" ]; then
        create_savepoints "$RUNNING_JOBS"
        echo
        
        # 8. åœæ­¢ä½œä¸š
        stop_jobs "$RUNNING_JOBS"
        echo
    fi
    
    # 9. å®‰è£…è¿æ¥å™¨
    install_connector
    echo
    
    # 10. é‡å¯é›†ç¾¤
    restart_cluster
    echo
    
    # 11. éªŒè¯å®‰è£…
    if verify_installation; then
        log_success "ğŸ‰ MySQL CDCè¿æ¥å™¨å®‰è£…å®Œæˆ!"
        echo
        echo "ğŸ“‹ æ¥ä¸‹æ¥çš„æ“ä½œæ­¥éª¤:"
        echo
        echo "ğŸ”´ 1. æ¢å¤çº¿ä¸Šä½œä¸š (ä¼˜å…ˆçº§æœ€é«˜!)"
        echo "   cd /home/ubuntu/work/script/flink_app/kafka2doris/scripts"
        echo "   flink sql-client -f kafka_to_doris_solution_sample.sql"
        echo
        if [ -n "$RUNNING_JOBS" ]; then
            echo "   ğŸ“„ è¯¦ç»†æ¢å¤æŒ‡ä»¤: cat $BACKUP_DIR/recovery_commands.sh"
            echo "   ğŸ“ Savepointä½ç½®: $BACKUP_DIR/savepoints/"
            echo
        fi
        echo "ğŸŸ¡ 2. éƒ¨ç½²æ–°çš„user_interestsä½œä¸š (åœ¨æ¢å¤ç°æœ‰ä½œä¸šå)"
        echo "   cd /home/ubuntu/work/script/flink_app/configs"
        echo "   flink sql-client -f user_interests_with_real_schema.sql"
        echo
        echo "ğŸ“Š 3. éªŒè¯æ‰€æœ‰ä½œä¸šçŠ¶æ€"
        echo "   flink list"
        echo "   curl http://localhost:8081/jobs"
        echo
        echo "ğŸ†˜ 4. å¦‚æœéœ€è¦æ¢å¤å¸®åŠ©"
        echo "   ./recovery_helper.sh $BACKUP_DIR"
    else
        log_error "å®‰è£…éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—"
        echo
        echo "ğŸ”§ å›æ»šæ“ä½œ:"
        echo "  1. æ¢å¤é…ç½®: cp -r $BACKUP_DIR/config/* $FLINK_HOME/conf/"
        echo "  2. æ¢å¤è¿æ¥å™¨: cp $BACKUP_DIR/lib/* $FLINK_HOME/lib/"
        echo "  3. é‡å¯é›†ç¾¤: $FLINK_HOME/bin/stop-cluster.sh && $FLINK_HOME/bin/start-cluster.sh"
        exit 1
    fi
}

# æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
case "${1:-}" in
    "install")
        main_install
        ;;
    "verify")
        verify_installation
        ;;
    "backup")
        backup_config
        ;;
    *)
        echo "MySQL CDCè¿æ¥å™¨å®‰è£…è„šæœ¬"
        echo
        echo "ç”¨æ³•:"
        echo "  $0 install  - å®‰è£…MySQL CDCè¿æ¥å™¨"
        echo "  $0 verify   - éªŒè¯è¿æ¥å™¨å®‰è£…"
        echo "  $0 backup   - ä»…å¤‡ä»½é…ç½®"
        echo
        echo "æ³¨æ„äº‹é¡¹:"
        echo "  - å®‰è£…å‰è¯·ç¡®ä¿å·²ä¸‹è½½è¿æ¥å™¨æ–‡ä»¶åˆ° $CDC_JAR"
        echo "  - å®‰è£…è¿‡ç¨‹ä¼šé‡å¯Flinké›†ç¾¤ï¼Œå½±å“è¿è¡Œä¸­çš„ä½œä¸š"
        echo "  - ä¼šè‡ªåŠ¨åˆ›å»ºsavepointç”¨äºä½œä¸šæ¢å¤"
        echo "  - å»ºè®®åœ¨ä¸šåŠ¡ä½å³°æœŸæ‰§è¡Œ"
        ;;
esac 