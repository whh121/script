#!/bin/bash
# Flinkä½œä¸šæ¢å¤è¾…åŠ©è„šæœ¬
# ======================
# 
# ç”¨é€”: ååŠ©ä»savepointæ¢å¤Flinkä½œä¸š
# ç‰¹æ€§: è‡ªåŠ¨æ£€æµ‹savepointã€æä¾›æ¢å¤å‘½ä»¤
# ä½œè€…: Flinkè¿ç»´å›¢é˜Ÿ
# æ›´æ–°: 2025-05-29

set -e

# é…ç½®å˜é‡
FLINK_WEB="http://localhost:8081"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# æŸ¥æ‰¾å¯ç”¨çš„savepoint
find_savepoints() {
    local backup_dir="$1"
    
    if [ ! -d "$backup_dir" ]; then
        log_error "å¤‡ä»½ç›®å½•ä¸å­˜åœ¨: $backup_dir"
        return 1
    fi
    
    log_info "åœ¨ $backup_dir æŸ¥æ‰¾savepoint..."
    
    if [ -d "$backup_dir/savepoints" ]; then
        echo
        echo "ğŸ“ å‘ç°çš„savepointç›®å½•:"
        ls -la "$backup_dir/savepoints/"
        echo
        
        # æŸ¥æ‰¾å…·ä½“çš„savepointè·¯å¾„
        find "$backup_dir/savepoints" -name "_metadata" 2>/dev/null | while read metadata_file; do
            savepoint_path=$(dirname "$metadata_file")
            echo "âœ… å¯ç”¨savepoint: $savepoint_path"
        done
    else
        log_warn "æœªæ‰¾åˆ°savepointç›®å½•"
    fi
}

# æ˜¾ç¤ºå¯ç”¨çš„SQLæ–‡ä»¶
show_sql_files() {
    log_info "ä½œä¸šSQLæ–‡ä»¶åˆ†ç±»:"
    echo
    
    echo "ğŸ”´ å½“å‰çº¿ä¸Šè¿è¡Œçš„ä½œä¸šSQL:"
    echo "  - flink_app/kafka2doris/scripts/kafka_to_doris_solution_sample.sql"
    echo "    â””â”€â”€ å¯¹åº”ä½œä¸š: Kafka â†’ Doris (client_cold_start)"
    echo "    â””â”€â”€ çŠ¶æ€: æ­£åœ¨è¿è¡Œ"
    echo
    
    echo "ğŸ“„ å…¶ä»–å¯ç”¨çš„ä½œä¸šSQL:"
    echo "  MySQL CDCä½œä¸š:"
    if [ -d "$PROJECT_ROOT/flink_app/mysql2doris/scripts" ]; then
        ls -la "$PROJECT_ROOT/flink_app/mysql2doris/scripts"/*.sql 2>/dev/null | sed 's/^/    /' || echo "    æ— SQLæ–‡ä»¶"
    fi
    echo
    
    echo "  Kafkaä½œä¸š:"
    if [ -d "$PROJECT_ROOT/flink_app/kafka2doris/scripts" ]; then
        ls -la "$PROJECT_ROOT/flink_app/kafka2doris/scripts"/*.sql 2>/dev/null | sed 's/^/    /' || echo "    æ— SQLæ–‡ä»¶"
    fi
    echo
    
    echo "ğŸŸ¡ å‡†å¤‡éƒ¨ç½²çš„æ–°ä½œä¸š:"
    echo "  - flink_app/configs/user_interests_with_real_schema.sql"
    echo "    â””â”€â”€ å¯¹åº”ä½œä¸š: MySQL CDC â†’ Doris (user_interests)"
    echo "    â””â”€â”€ çŠ¶æ€: å¾…éƒ¨ç½² (éœ€å…ˆå®‰è£…mysql-cdcè¿æ¥å™¨)"
    echo
}

# åˆ†æå¤‡ä»½ç›®å½•ä¸­çš„ä½œä¸šä¿¡æ¯
analyze_backup_jobs() {
    local backup_dir="$1"
    
    if [ ! -d "$backup_dir/job_analysis" ]; then
        log_warn "æœªæ‰¾åˆ°ä½œä¸šåˆ†æä¿¡æ¯ï¼Œå°†æä¾›é€šç”¨æ¢å¤æ–¹æ¡ˆ"
        return 1
    fi
    
    log_info "è¯»å–ä½œä¸šåˆ†æä¿¡æ¯..."
    
    if [ -f "$backup_dir/job_analysis/job_details.txt" ]; then
        echo
        echo "ğŸ“‹ åœæœºå‰è¿è¡Œçš„ä½œä¸šåˆ†æ:"
        echo "=========================="
        cat "$backup_dir/job_analysis/job_details.txt"
        echo
        
        # æå–æ¨èçš„SQLæ–‡ä»¶
        recommended_sqls=$(grep -A 10 "å¯èƒ½çš„SQLæ–‡ä»¶:" "$backup_dir/job_analysis/job_details.txt" | grep "flink_app" || echo "")
        
        if [ -n "$recommended_sqls" ]; then
            echo "ğŸ¯ æ¨èçš„æ¢å¤SQLæ–‡ä»¶:"
            echo "$recommended_sqls" | while read sql_file; do
                sql_file=$(echo "$sql_file" | sed 's/.*- //')
                if [ -f "$PROJECT_ROOT/$sql_file" ]; then
                    echo "  âœ… $sql_file (æ–‡ä»¶å­˜åœ¨)"
                else
                    echo "  âŒ $sql_file (æ–‡ä»¶ä¸å­˜åœ¨)"
                fi
            done
            echo
        fi
        
        return 0
    else
        return 1
    fi
}

# åˆ†æSQLåŒ¹é…ç»“æœ
analyze_sql_matches() {
    local backup_dir="$1"
    
    if [ ! -d "$backup_dir/sql_analysis" ]; then
        log_warn "æœªæ‰¾åˆ°SQLåŒ¹é…åˆ†æä¿¡æ¯"
        return 1
    fi
    
    log_info "åˆ†æSQLåŒ¹é…ç»“æœ..."
    
    # æ˜¾ç¤ºåŒ¹é…æŠ¥å‘Š
    if [ -f "$backup_dir/sql_analysis/sql_match_report.txt" ]; then
        echo
        echo "ğŸ” SQLå†…å®¹åŒ¹é…åˆ†ææŠ¥å‘Š:"
        echo "========================"
        cat "$backup_dir/sql_analysis/sql_match_report.txt"
        echo
    fi
    
    return 0
}

# è¿›è¡Œæ™ºèƒ½ç»„åˆä½œä¸šåŒ¹é…åˆ†æ
analyze_combined_job_matches() {
    local backup_dir="$1"
    
    if [ ! -d "$backup_dir/sql_analysis" ]; then
        log_warn "æœªæ‰¾åˆ°SQLåŒ¹é…åˆ†æä¿¡æ¯"
        return 1
    fi
    
    log_info "è¿›è¡Œæ™ºèƒ½ç»„åˆä½œä¸šåŒ¹é…åˆ†æ..."
    
    # åˆ›å»ºç»„åˆåˆ†ææŠ¥å‘Š
    echo "# æ™ºèƒ½ç»„åˆä½œä¸šåŒ¹é…æŠ¥å‘Š - $(date)" > "$backup_dir/sql_analysis/combined_match_report.txt"
    echo "# =======================================" >> "$backup_dir/sql_analysis/combined_match_report.txt"
    echo "" >> "$backup_dir/sql_analysis/combined_match_report.txt"
    
    # æ”¶é›†æ‰€æœ‰ä½œä¸šç‰¹å¾
    python3 -c "
import os, re, glob

# æ”¶é›†æ‰€æœ‰ä½œä¸šçš„ç‰¹å¾
all_job_features = {}
feature_files = glob.glob('$backup_dir/sql_analysis/*_features.txt')

for feature_file in feature_files:
    job_id = os.path.basename(feature_file).replace('_features.txt', '')
    try:
        with open(feature_file, 'r') as f:
            content = f.read()
            # æå–ç‰¹å¾éƒ¨åˆ†
            if 'ç‰¹å¾: ' in content:
                feature_str = content.split('ç‰¹å¾: ')[1].strip()
                features = set(feature_str.split('|')) if feature_str else set()
                all_job_features[job_id] = features
    except:
        continue

print(f'å‘ç° {len(all_job_features)} ä¸ªä½œä¸šéœ€è¦åˆ†æ')

# SQLæ–‡ä»¶ç‰¹å¾æå–
def extract_sql_features(sql_file):
    try:
        with open(sql_file, 'r', encoding='utf-8') as f:
            content = f.read()
    except:
        return set()
    
    features = set()
    
    # æ£€æŸ¥è¿æ¥å™¨ç±»å‹
    if \"'connector' = 'kafka'\" in content:
        features.add('connector:kafka')
    if \"'connector' = 'mysql-cdc'\" in content:
        features.add('connector:mysql-cdc')
    if \"'connector' = 'doris'\" in content:
        features.add('connector:doris')
    if \"'connector' = 'filesystem'\" in content:
        features.add('connector:filesystem')
    
    # æ£€æŸ¥æºè¡¨åï¼ˆCREATE TABLEï¼‰
    source_table_matches = re.findall(r'CREATE TABLE (\w+)', content, re.IGNORECASE)
    for table in source_table_matches:
        features.add(f'source_table:{table}')
    
    # æ£€æŸ¥ç›®æ ‡è¡¨åï¼ˆINSERT INTOï¼‰
    sink_table_matches = re.findall(r'INSERT INTO (\w+)', content, re.IGNORECASE)
    for table in sink_table_matches:
        features.add(f'sink_table:{table}')
    
    return features

# å€™é€‰SQLæ–‡ä»¶
sql_files = [
    '/home/ubuntu/work/script/flink_app/kafka2doris/scripts/kafka_to_doris_production.sql',
    '/home/ubuntu/work/script/flink_app/kafka2doris/scripts/kafka_to_doris_solution_sample.sql',
    '/home/ubuntu/work/script/flink_app/mysql2doris/scripts/mysql_sync.sql',
    '/home/ubuntu/work/script/flink_app/mysql2doris/scripts/mysql_content_audit_to_doris.sql'
]

# è®¡ç®—ç»„åˆåŒ¹é…åº¦
def calculate_combined_match(all_jobs_features, sql_features):
    # å°†æ‰€æœ‰ä½œä¸šç‰¹å¾åˆå¹¶
    combined_features = set()
    for job_features in all_jobs_features.values():
        combined_features.update(job_features)
    
    if not combined_features or not sql_features:
        return 0, {}
    
    intersection = combined_features.intersection(sql_features)
    union = combined_features.union(sql_features)
    
    # è®¡ç®—åŸºç¡€åŒ¹é…åº¦
    base_score = len(intersection) / len(union) if union else 0
    
    # è®¡ç®—sinkè¡¨åŒ¹é…åº¦ï¼ˆå…³é”®æŒ‡æ ‡ï¼‰
    job_sink_tables = {f for f in combined_features if f.startswith('sink_table:')}
    sql_sink_tables = {f for f in sql_features if f.startswith('sink_table:')}
    sink_match_count = len(job_sink_tables.intersection(sql_sink_tables))
    sink_total = len(job_sink_tables)
    
    sink_coverage = sink_match_count / sink_total if sink_total > 0 else 0
    
    # ç»¼åˆè¯„åˆ†: åŸºç¡€åŒ¹é…åº¦ + sinkè¡¨è¦†ç›–åº¦æƒé‡
    final_score = base_score * 0.6 + sink_coverage * 0.4
    
    match_details = {
        'base_score': base_score,
        'sink_coverage': sink_coverage,
        'sink_matches': sink_match_count,
        'sink_total': sink_total,
        'intersection': sorted(intersection),
        'job_features': sorted(combined_features),
        'sql_features': sorted(sql_features)
    }
    
    return final_score, match_details

print('\\nğŸ¯ ç»„åˆä½œä¸šåŒ¹é…åˆ†æ:')
print('=' * 60)

best_matches = []
for sql_file in sql_files:
    if os.path.exists(sql_file):
        sql_features = extract_sql_features(sql_file)
        score, details = calculate_combined_match(all_job_features, sql_features)
        
        if score > 0:
            best_matches.append((sql_file, score, details))

# æ’åºå¹¶è¾“å‡ºè¯¦ç»†åˆ†æ
best_matches.sort(key=lambda x: x[1], reverse=True)

for sql_file, score, details in best_matches:
    filename = sql_file.split('/')[-1]
    print(f'\\nğŸ“„ {filename}:')
    print(f'  ç»¼åˆåŒ¹é…åº¦: {score:.2f}')
    print(f'  åŸºç¡€åŒ¹é…åº¦: {details[\"base_score\"]:.2f}')
    print(f'  Sinkè¡¨è¦†ç›–: {details[\"sink_coverage\"]:.2f} ({details[\"sink_matches\"]}/{details[\"sink_total\"]})')
    
    if details['sink_matches'] > 0:
        job_sinks = [f.split(':')[1] for f in details['job_features'] if f.startswith('sink_table:')]
        sql_sinks = [f.split(':')[1] for f in details['sql_features'] if f.startswith('sink_table:')]
        matched_sinks = set(job_sinks).intersection(set(sql_sinks))
        print(f'  åŒ¹é…çš„è¡¨: {sorted(matched_sinks)}')

if best_matches:
    print(f'\\nğŸ† æœ€ä½³åŒ¹é…: {best_matches[0][0].split(\"/\")[-1]} (åŒ¹é…åº¦: {best_matches[0][1]:.2f})')
    
    # åˆ¤æ–­åŒ¹é…è´¨é‡
    best_score = best_matches[0][1]
    if best_score >= 0.8:
        print('âœ… é«˜ç½®ä¿¡åº¦åŒ¹é… - å¼ºçƒˆæ¨èä½¿ç”¨æ­¤SQLæ–‡ä»¶æ¢å¤')
    elif best_score >= 0.6:
        print('âœ… è‰¯å¥½åŒ¹é… - æ¨èä½¿ç”¨æ­¤SQLæ–‡ä»¶æ¢å¤')
    elif best_score >= 0.4:
        print('âš ï¸  ä¸­ç­‰åŒ¹é… - å»ºè®®éªŒè¯åä½¿ç”¨')
    else:
        print('âŒ ä½åŒ¹é…åº¦ - å»ºè®®ä»é…ç½®æ–‡ä»¶é‡å»º')
else:
    print('\\nâŒ æœªæ‰¾åˆ°åˆé€‚çš„åŒ¹é…æ–‡ä»¶')
"
    
    return 0
}

# ç”ŸæˆåŸºäºåŒ¹é…ç»“æœçš„æ¢å¤é€‰é¡¹
generate_match_based_recovery() {
    local backup_dir="$1"
    
    echo "ğŸ¯ åŸºäºSQLåŒ¹é…çš„ç²¾ç¡®æ¢å¤æ–¹æ¡ˆ:"
    echo "========================================="
    echo
    
    if [ ! -d "$backup_dir/sql_analysis" ]; then
        echo "âŒ æœªæ‰¾åˆ°SQLåŒ¹é…åˆ†æï¼Œä½¿ç”¨é€šç”¨æ–¹æ¡ˆ"
        return 1
    fi
    
    # å¤„ç†æ¯ä¸ªä½œä¸šçš„åŒ¹é…ç»“æœ
    for match_file in "$backup_dir/sql_analysis"/*_match.txt; do
        if [ -f "$match_file" ]; then
            job_id=$(basename "$match_file" _match.txt)
            
            echo "ğŸ“‹ ä½œä¸š $job_id æ¢å¤é€‰é¡¹:"
            echo "----------------------------------------"
            
            # è¯»å–åŒ¹é…ç»“æœ
            match_content=$(cat "$match_file")
            
            # è§£ææœ€ä½³åŒ¹é…
            best_match=$(echo "$match_content" | grep -A 1 "^---$" | tail -n +2 | head -n 1)
            
            if [ -n "$best_match" ]; then
                sql_file=$(echo "$best_match" | cut -d':' -f1)
                score=$(echo "$best_match" | cut -d':' -f2)
                
                # åˆ¤æ–­åŒ¹é…è´¨é‡
                match_quality=$(python3 -c "
score = float('$score')
if score >= 0.8:
    print('excellent')
elif score >= 0.6:
    print('good')
elif score >= 0.3:
    print('fair')
else:
    print('poor')
")
                
                echo "ğŸ¯ åŒ¹é…ç»“æœ: $sql_file (åŒ¹é…åº¦: $score)"
                echo
                
                case $match_quality in
                    "excellent"|"good")
                        echo "âœ… é«˜ç½®ä¿¡åº¦åŒ¹é…! æ¨èæ¢å¤é€‰é¡¹:"
                        echo
                        echo "é€‰é¡¹1: ä½¿ç”¨åŒ¹é…çš„SQLæ–‡ä»¶æ¢å¤ (æ¨è)"
                        echo "  cd $(dirname "$sql_file")"
                        echo "  flink sql-client -f $(basename "$sql_file")"
                        echo
                        echo "é€‰é¡¹2: ä½¿ç”¨ä¿å­˜çš„ä½œä¸šé…ç½®æ¢å¤"
                        echo "  # ä»å¤‡ä»½é…ç½®é‡å»ºä½œä¸š (éœ€è¦æ‰‹åŠ¨æ„å»ºSQL)"
                        echo "  # é…ç½®æ–‡ä»¶: $backup_dir/sql_analysis/${job_id}_config.json"
                        ;;
                    "fair")
                        echo "âš ï¸  ä¸­ç­‰åŒ¹é…åº¦ï¼Œå»ºè®®å¯¹æ¯”éªŒè¯:"
                        echo
                        echo "é€‰é¡¹1: éªŒè¯åä½¿ç”¨åŒ¹é…çš„SQLæ–‡ä»¶"
                        echo "  # å…ˆæ£€æŸ¥: cat $sql_file"
                        echo "  # å¯¹æ¯”é…ç½®: cat $backup_dir/sql_analysis/${job_id}_signature.txt"
                        echo "  cd $(dirname "$sql_file")"
                        echo "  flink sql-client -f $(basename "$sql_file")"
                        echo
                        echo "é€‰é¡¹2: ä½¿ç”¨ä¿å­˜çš„ä½œä¸šé…ç½®é‡å»º (å®‰å…¨)"
                        echo "  # ä»é…ç½®æ–‡ä»¶æ‰‹åŠ¨é‡å»ºSQL"
                        echo "  # é…ç½®å‚è€ƒ: $backup_dir/sql_analysis/${job_id}_config.json"
                        ;;
                    "poor")
                        echo "âŒ åŒ¹é…åº¦è¾ƒä½ï¼Œå»ºè®®æ‰‹åŠ¨é‡å»º:"
                        echo
                        echo "æ¨èé€‰é¡¹: ä½¿ç”¨ä¿å­˜çš„ä½œä¸šé…ç½®é‡å»º"
                        echo "  # é…ç½®æ–‡ä»¶: $backup_dir/sql_analysis/${job_id}_config.json"
                        echo "  # ä½œä¸šç­¾å: $backup_dir/sql_analysis/${job_id}_signature.txt"
                        echo "  # æ‰‹åŠ¨æ„å»ºå¯¹åº”çš„SQLæ–‡ä»¶"
                        echo
                        echo "å¤‡é€‰: æ£€æŸ¥å¯èƒ½åŒ¹é… (éœ€éªŒè¯)"
                        echo "  # ä½åŒ¹é…: $sql_file"
                        echo "  # è¯·ä»”ç»†å¯¹æ¯”åå†ä½¿ç”¨"
                        ;;
                esac
            else
                echo "âŒ æœªæ‰¾åˆ°åŒ¹é…çš„SQLæ–‡ä»¶"
                echo
                echo "æ¨èæ“ä½œ: ä»ä¿å­˜çš„é…ç½®é‡å»ºä½œä¸š"
                echo "  # ä½œä¸šé…ç½®: $backup_dir/sql_analysis/${job_id}_config.json"
                echo "  # ä½œä¸šç­¾å: $backup_dir/sql_analysis/${job_id}_signature.txt"
                echo "  # æ‰§è¡Œè®¡åˆ’: $backup_dir/sql_analysis/${job_id}_plan.json"
            fi
            
            echo
            echo "ğŸ”§ é…ç½®æ–‡ä»¶ä½ç½®:"
            echo "  - å®Œæ•´é…ç½®: $backup_dir/sql_analysis/${job_id}_config.json"
            echo "  - å…³é”®ç­¾å: $backup_dir/sql_analysis/${job_id}_signature.txt"  
            echo "  - æ‰§è¡Œè®¡åˆ’: $backup_dir/sql_analysis/${job_id}_plan.json"
            echo
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo
        fi
    done
}

# ç”Ÿæˆé…ç½®æ–‡ä»¶é‡å»ºæŒ‡å¯¼
generate_config_rebuild_guide() {
    local backup_dir="$1"
    
    echo "ğŸ”¨ ä»é…ç½®æ–‡ä»¶é‡å»ºä½œä¸šæŒ‡å¯¼:"
    echo "================================="
    echo
    echo "å¦‚æœåŒ¹é…åº¦ä¸é«˜æˆ–éœ€è¦ç²¾ç¡®é‡å»ºï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•:"
    echo
    echo "1. æŸ¥çœ‹ä¿å­˜çš„ä½œä¸šé…ç½®:"
    echo "   cat $backup_dir/sql_analysis/JOB_ID_config.json | jq '.'"
    echo
    echo "2. æå–å…³é”®é…ç½®ä¿¡æ¯:"
    echo "   cat $backup_dir/sql_analysis/JOB_ID_signature.txt"
    echo
    echo "3. æ ¹æ®é…ç½®é‡å»ºSQLæ–‡ä»¶:"
    echo "   - ä» connector é…ç½®é‡å»º CREATE TABLE è¯­å¥"
    echo "   - ä» table.identifier ç¡®å®šç›®æ ‡è¡¨"
    echo "   - ä» topic/hostname ç¡®å®šæ•°æ®æº"
    echo
    echo "4. ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå™¨ (å¦‚æœé€‚ç”¨):"
    echo "   cd $PROJECT_ROOT/flink_app/configs"
    echo "   # æ ¹æ®é…ç½®ç±»å‹é€‰æ‹©åˆé€‚çš„æ¨¡æ¿ç”Ÿæˆå‘½ä»¤"
    echo
    echo "ğŸ’¡ é‡å»ºæç¤º:"
    echo "  - ä¿å­˜çš„é…ç½®åŒ…å«äº†ä½œä¸šçš„å®Œæ•´è¿è¡Œæ—¶å‚æ•°"
    echo "  - å¯ä»¥ä½œä¸ºé‡æ–°ç¼–å†™SQLçš„å‡†ç¡®å‚è€ƒ"
    echo "  - ç¡®ä¿æ–°SQLçš„è¿æ¥å™¨é…ç½®ä¸åŸé…ç½®ä¸€è‡´"
}

# ç”ŸæˆåŠ¨æ€æ¢å¤å‘½ä»¤
generate_dynamic_recovery_commands() {
    local backup_dir="$1"
    
    echo "ğŸ”§ åŠ¨æ€ä½œä¸šæ¢å¤æ–¹æ³•:"
    echo
    
    # å¦‚æœæœ‰ä½œä¸šåˆ†æä¿¡æ¯ï¼Œä½¿ç”¨åˆ†æç»“æœ
    if analyze_backup_jobs "$backup_dir" >/dev/null 2>&1; then
        echo "ğŸ“ˆ åŸºäºä½œä¸šåˆ†æçš„æ¢å¤å»ºè®®:"
        echo "--------------------------------------"
        
        # ä»åˆ†ææ–‡ä»¶ä¸­æå–æ¨èçš„SQL
        if [ -f "$backup_dir/job_analysis/job_details.txt" ]; then
            recommended_sqls=$(grep -A 10 "å¯èƒ½çš„SQLæ–‡ä»¶:" "$backup_dir/job_analysis/job_details.txt" | grep "flink_app" | sed 's/.*- //' || echo "")
            
            if [ -n "$recommended_sqls" ]; then
                echo "$recommended_sqls" | while read sql_file; do
                    if [ -f "$PROJECT_ROOT/$sql_file" ]; then
                        echo "# æ¢å¤ä½œä¸š: $(basename "$sql_file" .sql)"
                        echo "cd $PROJECT_ROOT/$(dirname "$sql_file")"
                        echo "flink sql-client -f $(basename "$sql_file")"
                        echo
                    fi
                done
            else
                echo "# æœªæ‰¾åˆ°åŒ¹é…çš„SQLæ–‡ä»¶ï¼Œè¯·æ‰‹åŠ¨ç¡®è®¤"
            fi
        fi
    else
        echo "âš ï¸  æ— ä½œä¸šåˆ†æä¿¡æ¯ï¼Œæä¾›é€šç”¨æ¢å¤æ–¹æ¡ˆ:"
        echo "--------------------------------------"
        echo "# æ£€æŸ¥å¯ç”¨çš„SQLæ–‡ä»¶å¹¶æ‰‹åŠ¨é€‰æ‹©"
        echo "ls -la $PROJECT_ROOT/flink_app/*/scripts/*.sql"
        echo
        echo "# å¸¸è§çš„æ¢å¤å‘½ä»¤ï¼š"
        echo "cd $PROJECT_ROOT/flink_app/kafka2doris/scripts"
        echo "flink sql-client -f kafka_to_doris_solution_sample.sql"
        echo
        echo "cd $PROJECT_ROOT/flink_app/mysql2doris/scripts"  
        echo "flink sql-client -f mysql_sync.sql"
        echo
    fi
}

# æ£€æŸ¥å½“å‰é›†ç¾¤çŠ¶æ€
check_cluster_status() {
    log_info "æ£€æŸ¥Flinké›†ç¾¤çŠ¶æ€..."
    
    if curl -s "$FLINK_WEB/jobs" >/dev/null; then
        log_success "Flinké›†ç¾¤è¿è¡Œæ­£å¸¸"
        
        # æ˜¾ç¤ºå½“å‰è¿è¡Œçš„ä½œä¸š
        jobs=$(curl -s "$FLINK_WEB/jobs" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    running_jobs = [job for job in data.get('jobs', []) if job['status'] == 'RUNNING']
    if running_jobs:
        print(f'å½“å‰è¿è¡Œ {len(running_jobs)} ä¸ªä½œä¸š:')
        for job in running_jobs:
            print(f'  - {job[\"id\"]} ({job[\"status\"]})')
    else:
        print('å½“å‰æ²¡æœ‰è¿è¡Œçš„ä½œä¸š')
except:
    print('æ— æ³•è§£æä½œä¸šçŠ¶æ€')
")
        echo "$jobs"
    else
        log_error "æ— æ³•è¿æ¥åˆ°Flinké›†ç¾¤"
        return 1
    fi
}

# ä¸»åŠŸèƒ½
main() {
    local backup_dir="$1"
    
    echo "ğŸ”„ Flinkä½œä¸šæ¢å¤è¾…åŠ©å·¥å…·"
    echo "=========================="
    echo
    
    # æ£€æŸ¥é›†ç¾¤çŠ¶æ€
    check_cluster_status
    echo
    
    # å¦‚æœæä¾›äº†å¤‡ä»½ç›®å½•ï¼Œåˆ†æä½œä¸šä¿¡æ¯
    if [ -n "$backup_dir" ]; then
        if analyze_backup_jobs "$backup_dir"; then
            echo "âœ… æˆåŠŸè¯»å–ä½œä¸šåˆ†æä¿¡æ¯"
        else
            log_warn "æ— æ³•è¯»å–ä½œä¸šåˆ†æä¿¡æ¯ï¼Œå°†æä¾›é€šç”¨æ–¹æ¡ˆ"
        fi
        
        # æŸ¥æ‰¾savepoint
        find_savepoints "$backup_dir"
        echo
    fi
    
    # æ˜¾ç¤ºå¯ç”¨çš„SQLæ–‡ä»¶
    show_sql_files
    
    # åˆ†æSQLåŒ¹é…ç»“æœ
    analyze_sql_matches "$backup_dir"
    
    # è¿›è¡Œæ™ºèƒ½ç»„åˆä½œä¸šåŒ¹é…åˆ†æ
    analyze_combined_job_matches "$backup_dir"
    
    # ç”ŸæˆåŸºäºåŒ¹é…ç»“æœçš„æ¢å¤é€‰é¡¹
    generate_match_based_recovery "$backup_dir"
    
    # ç”Ÿæˆé…ç½®æ–‡ä»¶é‡å»ºæŒ‡å¯¼
    generate_config_rebuild_guide "$backup_dir"
    
    # æ·»åŠ é€šç”¨æ“ä½œæŒ‡å¯¼
    echo
    echo "ğŸŸ¡ éƒ¨ç½²æ–°çš„user_interestsä½œä¸š:"
    echo "--------------------------------------"
    echo "# ç¡®è®¤MySQL CDCè¿æ¥å™¨å·²å®‰è£…"
    echo "cd $PROJECT_ROOT/flink_app/configs"
    echo "./config_validator.py --job mysql2doris --table user_interests"
    echo
    echo "# éƒ¨ç½²æ–°ä½œä¸š"
    echo "flink sql-client -f user_interests_with_real_schema.sql"
    echo
    
    echo "ğŸ“Š éªŒè¯å’Œç›‘æ§:"
    echo "--------------------------------------"
    echo "flink list                           # æŸ¥çœ‹æ‰€æœ‰ä½œä¸š"
    echo "curl http://localhost:8081/jobs | jq # æŸ¥çœ‹è¯¦ç»†çŠ¶æ€"
    echo
    
    echo "ğŸ’¡ æ¢å¤å»ºè®®:"
    echo "1. ä¼˜å…ˆæ¢å¤åˆ†ææŠ¥å‘Šä¸­æ¨èçš„SQLæ–‡ä»¶"
    echo "2. éªŒè¯æ¢å¤çš„ä½œä¸šæ•°æ®æµæ­£å¸¸"
    echo "3. ç¡®è®¤æ‰€æœ‰ä½œä¸šçŠ¶æ€ä¸ºRUNNING"
    echo "4. éƒ¨ç½²æ–°ä½œä¸šå‰ç¡®è®¤ç°æœ‰ä½œä¸šç¨³å®š"
}

# å‘½ä»¤è¡Œå‚æ•°å¤„ç†
case "${1:-}" in
    "")
        echo "Flinkä½œä¸šæ¢å¤è¾…åŠ©è„šæœ¬"
        echo
        echo "ç”¨æ³•:"
        echo "  $0 <backup-directory>  - ä»æŒ‡å®šå¤‡ä»½ç›®å½•æ¢å¤"
        echo "  $0 check               - ä»…æ£€æŸ¥é›†ç¾¤çŠ¶æ€"
        echo "  $0 list                - åˆ—å‡ºå¯ç”¨çš„SQLæ–‡ä»¶"
        echo
        echo "ç¤ºä¾‹:"
        echo "  $0 /tmp/flink_backup_20250529_083736"
        echo "  $0 check"
        ;;
    "check")
        check_cluster_status
        ;;
    "list")
        show_sql_files
        ;;
    *)
        main "$1"
        ;;
esac 